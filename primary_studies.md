# Primary studies

**P1:** M. S. S. Junior and N. S. Rosa and F. A. A. Lins, Execution Support to Long Running Workflows. International Conference on Computer and Information Technology, 2014. [https://doi.org/10.1109/CIT.2014.94](https://doi.org/10.1109/CIT.2014.94)

**P2:** T. Tsai and K. Vaidyanathan and K. Gross, Low-Overhead Run-Time Memory Leak Detection and Recovery. Pacific Rim International Symposium on Dependable Computing, 2006. [https://doi.org/10.1109/PRDC.2006.42](https://doi.org/10.1109/PRDC.2006.42)

**P3:** Y. Bao and M. Chen and Q. Zhu and T. Wei and F. Mallet and T. Zhou, Quantitative Performance Evaluation of Uncertainty-Aware Hybrid AADL Designs Using Statistical Model Checking. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2017. [https://doi.org/10.1109/TCAD.2017.2681076](https://doi.org/10.1109/TCAD.2017.2681076)

**P4:** R. J. Pooley and A. A. L. Abdullatif, CPASA: Continuous Performance Assessment of Software Architecture. International Conference and Workshop on Engineering of Computer-Based Systems, 2010. [https://doi.org/10.1109/ECBS.2010.16](https://doi.org/10.1109/ECBS.2010.16)

**P5:** Wert, Alexander and Schulz, Henning and Heger, Christoph, AIM: Adaptable Instrumentation and Monitoring for Automated Software Performance Analysis. International Workshop on Automation of Software Test, 2015. [https://doi.org/10.1109/AST.2015.15](https://doi.org/10.1109/AST.2015.15)

**P6:** Voneva, Sonyaand Mazkatli, Manarand Grohmann, Johannesand Koziolek, Anne, Optimizing Parametric Dependencies for Incremental Performance Model Extraction. European Conference on Software Architecture, 2020. [https://doi.org/10.1007/978-3-030-59155-7_17](https://doi.org/10.1007/978-3-030-59155-7_17)

**P7:** Bernardi, Simona and Domínguez, Juan L.and Gómez, Abeland Joubert, Christopheand Merseguer, Joséand Perez-Palacin, Diegoand Requeno, José I.and Romeu, Alberto, A systematic approach for performance assessment using process mining. Empirical Software Engineering, 2018. [https://doi.org/10.1007/s10664-018-9606-9](https://doi.org/10.1007/s10664-018-9606-9)

**P8:** Falkner, Katrinaand Szabo, Claudiaand Chiprianov, Vaneaand Puddy, Gavinand Rieckmann, Marianneand Fraser, Danand Aston, Cathlyn, Model-driven performance prediction of systems of systems. Software Systems Modeling, 2018. [https://doi.org/10.1007/s10270-016-0547-8](https://doi.org/10.1007/s10270-016-0547-8)

**P9:** Ehlers, Jensand Hasselbring, Wilhelm, A Self-adaptive Monitoring Framework for Component-Based Software Systems. European Conference on Software Architecture, 2011. [https://doi.org/10.1007/978-3-642-23798-0_30](https://doi.org/10.1007/978-3-642-23798-0_30)

**P10:** Vögele, Christianand van Hoorn, Andréand Schulz, Eikeand Hasselbring, Wilhelmand Krcmar, Helmut, WESSBAS: extraction of probabilistic workload specifications for load testing and performance prediction---a model-driven approach for session-based application systems. Software Systems Modeling, 2018. [https://doi.org/10.1007/s10270-016-0566-5](https://doi.org/10.1007/s10270-016-0566-5)

**P11:** Giaimo, Federicoand Berger, Christian, Continuous Experimentation for Automotive Software on the Example of a Heavy Commercial Vehicle in Daily Operation. European Conference on Software Architecture, 2020. [https://doi.org/10.1007/978-3-030-58923-3_5](https://doi.org/10.1007/978-3-030-58923-3_5)

**P12:** Cholomskis, Aurimasand Pozdniakova, Olesiaand Mažeika, Dalius, Cloud Software Performance Metrics Collection and Aggregation for Auto-Scaling Module. International Conference on Information and Software Technologies, 2018. [https://doi.org/10.1007/978-3-319-99972-2_10](https://doi.org/10.1007/978-3-319-99972-2_10)

**P13:** Li, Chenand Altamimi, Taghreedand Zargari, Mana Hassanzadehand Casale, Giulianoand Petriu, Dorina, Tulsa: A Tool for Transforming UML to Layered Queueing Networks for Performance Analysis of Data Intensive Applications. Quantitative Evaluation of Systems, 2017. [https://doi.org/10.1007/978-3-319-66335-7_18](https://doi.org/10.1007/978-3-319-66335-7_18)

**P14:** Willnecker, F. and Krcmar, H., Model-based prediction of automatic memory management and garbage collection behavior. Simulation Modelling Practice and Theory, 2019. [https://doi.org/10.1016/j.simpat.2018.09.014](https://doi.org/10.1016/j.simpat.2018.09.014)

**P15:** Muller, H. and Bosse, S. and Turowski, K., On the utility of machine learning for service capacity management of enterprise applications. International Conference on Signal Image Technology and Internet Based Systems, 2019. [https://doi.org/10.1109/SITIS.2019.00053](https://doi.org/10.1109/SITIS.2019.00053)

**P16:** Del Rosso, C., Continuous evolution through software architecture evaluation: A case study. Journal of Software Maintenance and Evolution, 2006. [https://doi.org/10.1002/smr.337](https://doi.org/10.1002/smr.337)

**P17:** Castellanos, C. and Varela, C.A. and Correal, D., Measuring performance quality scenarios in big data analytics applications: A DevOps and domain-specific model approach. ACM International Conference Proceeding Series, 2019. [https://doi.org/10.1145/3344948.3344986](https://doi.org/10.1145/3344948.3344986)

**P18:** Grohmann, J. and Eismann, S. and Elflein, S. and Kistowski, J.V. and Kounev, S. and Mazkatli, M., Detecting parametric dependencies for performance models using feature selection techniques. Annual International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunications Systems, MASCOTS, 2019. [https://doi.org/10.1109/MASCOTS.2019.00042](https://doi.org/10.1109/MASCOTS.2019.00042)

**P19:** Perez-Palacin, D. and Merseguer, J. and Requeno, J.I. and Guerriero, M. and Di Nitto, E. and Tamburri, D.A., A UML Profile for the Design, Quality Assessment and Deployment of Data-intensive Applications. Software and Systems Modeling, 2019. [https://doi.org/10.1007/s10270-019-00730-3](https://doi.org/10.1007/s10270-019-00730-3)

**P20:** Del Rosso, C., Software performance tuning of software product family architectures: Two case studies in the real-time embedded systems domain. Journal of Systems and Software, 2008. [https://doi.org/10.1016/j.jss.2007.07.006](https://doi.org/10.1016/j.jss.2007.07.006)

**P21:** Von Massow, R. and Van Hoorn, A. and Hasselbring, W., Performance simulation of runtime reconfigurable component-based software architectures. European Conference on Software Architecture, 2011. [https://doi.org/10.1007/978-3-642-23798-0_5](https://doi.org/10.1007/978-3-642-23798-0_5)

**P22:** Arcelli, D. and Cortellessa, V. and Di Pompeo, D. and Eramo, R. and Tucci, M., Exploiting architecture/runtime model-driven traceability for performance improvement. International Conference on Software Architecture, 2019. [https://doi.org/10.1109/ICSA.2019.00017](https://doi.org/10.1109/ICSA.2019.00017)

**P23:** Incerto, E. and Tribastone, M. and Trubiani, C., A proactive approach for runtime self-adaptation based on queueing network fluid analysis. 1st International Workshop on Quality-Aware DevOps, QUDOS 2015, 2015. [https://doi.org/10.1145/2804371.2804375](https://doi.org/10.1145/2804371.2804375)

**P24:** Bezemer, C.-P. and Eismann, S. and Ferme, V. and Grohmann, J. and Heinrich, R. and Jamshidi, P. and Shang, W. and Van Hoorn, A. and Villavicencio, M. and Walter, J. and Willnecker, F., How is performance addressed in DevOps? A survey on industrial practices. International Conference on Performance Engineering, 2019. [https://doi.org/10.1145/3297663.3309672](https://doi.org/10.1145/3297663.3309672)

**P25:** De Sanctis, M. and Bucchiarone, A. and Trubiani, C., A DevOps Perspective for QoS-Aware Adaptive Applications. International Workshop on Software Engineering Aspects of Continuous Development and New Paradigms of Software Production and Deployment, 2020. [https://doi.org/10.1007/978-3-030-39306-9_7](https://doi.org/10.1007/978-3-030-39306-9_7)

**P26:** Mazkatli, M. and Monschein, D. and Grohmann, J. and Koziolek, A., Incremental calibration of architectural performance models with parametric dependencies. International Conference on Software Architecture, 2020. [https://doi.org/10.1109/ICSA47634.2020.00011](https://doi.org/10.1109/ICSA47634.2020.00011)

**P27:** Incerto, E. and Tribastone, M. and Trubiani, C., Software performance self-adaptation through efficient model predictive control. International Conference on Automated Software Engineering, 2017. [https://doi.org/10.1109/ASE.2017.8115660](https://doi.org/10.1109/ASE.2017.8115660)

**P28:** Brebner, P., Automatic performance modelling from application performance management (APM) data: An experience report. International Conference on Performance Engineering, 2016. [https://doi.org/10.1145/2851553.2851560](https://doi.org/10.1145/2851553.2851560)

**P29:** Willnecker, F. and Krcmar, H., Multi-objective optimization of deployment topologies for distributed applications. ACM Transactions on Internet Technology, 2018. [https://doi.org/10.1145/3106158](https://doi.org/10.1145/3106158)

**P30:** Pitakrat, T. and Okanović, D. and van Hoorn, A. and Grunske, L., Hora: Architecture-aware online failure prediction. Journal of Systems and Software, 2018. [https://doi.org/10.1016/j.jss.2017.02.041](https://doi.org/10.1016/j.jss.2017.02.041)

**P31:** Trubiani, C. and Bran, A. and van Hoorn, A. and Avritzer, A. and Knoche, H., Exploiting load testing and profiling for Performance Antipattern Detection. Information and Software Technology, 2018. [https://doi.org/10.1016/j.infsof.2017.11.016](https://doi.org/10.1016/j.infsof.2017.11.016)

**P32:** Aleti, A. and Trubiani, C. and van Hoorn, A. and Jamshidi, P., An efficient method for uncertainty propagation in robust software performance estimation. Journal of Systems and Software, 2018. [https://doi.org/10.1016/j.jss.2018.01.010](https://doi.org/10.1016/j.jss.2018.01.010)

**P33:** Spinnner, S. and Grohmann, J. and Eismann, S. and Kounev, S., Online model learning for self-aware computing infrastructures. Journal of Systems and Software, 2019. [https://doi.org/10.1016/j.jss.2018.09.089](https://doi.org/10.1016/j.jss.2018.09.089)

**P34:** Yasaweerasinghelage, R. and Staples, M. and Paik, H.-Y. and Weber, I., Optimising architectures for performance, cost, and security. European Conference on Software Architecture, 2019. [https://doi.org/10.1007/978-3-030-29983-5_11](https://doi.org/10.1007/978-3-030-29983-5_11)

**P35:** Trubiani, C. and Jamshidi, P. and Cito, J. and Shang, W. and Jiang, Z.M. and Borg, M., Performance issues? Hey DevOps, mind the uncertainty. IEEE Software, 2019. [https://doi.org/10.1109/MS.2018.2875989](https://doi.org/10.1109/MS.2018.2875989)

**P36:** Incerto, E. and Tribastone, M. and Trubiani, C., Symbolic performance adaptation. International Symposium on Software Engineering for Adaptive and Self-Managing Systems, 2016. [https://doi.org/10.1145/2897053.2897060](https://doi.org/10.1145/2897053.2897060)

**P37:** Willnecker, F. and Krcmar, H., Optimization of deployment topologies for distributed enterprise applications. International ACM SIGSOFT Conference on Quality of Software Architectures, 2016. [https://doi.org/10.1109/QoSA.2016.11](https://doi.org/10.1109/QoSA.2016.11)

**P38:** Gerostathopoulos, I. and Bures, T. and Schmid, S. and Horky, V. and Prehofer, C. and Tuma, P., Towards systematic live experimentation in software-intensive systems of systems. International Colloquium on Software-intensive Systems-of-Systems, 2016. [https://doi.org/10.1145/3175731.3176175](https://doi.org/10.1145/3175731.3176175)

**P39:** Keck, P. and Hoorn, A.V. and Okanovic, D. and Pitakrat, T. and Dullmann, T.F., Antipattern-Based Problem Injection for Assessing Performance and Reliability Evaluation Techniques. International Conference on Software Reliability Engineering Workshops, 2016. [https://doi.org/10.1109/ISSREW.2016.36](https://doi.org/10.1109/ISSREW.2016.36)

**P40:** Walter, J. and Stier, C. and Koziolek, H. and Kounev, S., An expandable extraction framework for architectural performance models. Companion International Conference on Performance Engineering, 2017. [https://doi.org/10.1145/3053600.3053634](https://doi.org/10.1145/3053600.3053634)

**P41:** Kunz, J. and Heger, C. and Heinrich, R., A generic platform for transforming monitoring data into performance models. Companion International Conference on Performance Engineering, 2017. [https://doi.org/10.1145/3053600.3053635](https://doi.org/10.1145/3053600.3053635)

**P42:** Heinrich, R., Architectural runtime models for integrating runtime observations and component-based models. Journal of Systems and Software, 2020. [https://doi.org/10.1016/j.jss.2020.110722](https://doi.org/10.1016/j.jss.2020.110722)

**P43:** Trubiani, C. and Mirandola, R., Continuous rearchitecting of QoS models: Collaborative analysis for uncertainty reduction. European Conference on Software Architecture, 2017. [https://doi.org/10.1007/978-3-319-65831-5_3](https://doi.org/10.1007/978-3-319-65831-5_3)

**P44:** Mazkatli, M. and Koziolek, A., Continuous integration of performance model. Companion International Conference on Performance Engineering, 2018. [https://doi.org/10.1145/3185768.3186285](https://doi.org/10.1145/3185768.3186285)

**P45:** von Laszewski, Gregor and Lee, Hyungro and Diaz, Javier and Wang, Fugang and Tanaka, Koji and Karavinkoppa, Shubhada and Fox, Geoffrey C. and Furlani, Tom, Design of an Accounting and Metric-Basedcloud-Shifting and Cloud-Seeding Framework for Federatedclouds and Bare-Metal Environments. workshop on Cloud services, federation, and the 8th open cirrus summit, 2012. [https://doi.org/10.1145/2378975.2378982](https://doi.org/10.1145/2378975.2378982)

**P46:** Birngruber, Erich and Forai, Petar and Zauner, Aaron, Total Recall: Holistic Metrics for Broad Systems Performance and User Experience Visibility in a Data-Intensive Computing Environment. International Workshop on HPC User Support Tools, 2015. [https://doi.org/10.1145/2834996.2835001](https://doi.org/10.1145/2834996.2835001)

**P47:** Wert, Alexander, Performance Problem Diagnostics by Systematic Experimentation. CompArch: Component-Based Software Engineering and Software Architecture, 2013. [https://doi.org/10.1145/2465498.2465499](https://doi.org/10.1145/2465498.2465499)

**P48:** Heinrich, Robert and van Hoorn, André and Knoche, Holger and Li, Fei and Lwakatare, Lucy Ellen and Pahl, Claus and Schulte, Stefan and Wettinger, Johannes, Performance Engineering for Microservices: Research Challenges and Directions. Companion International Conference on Performance Engineering Companion, 2017. [https://doi.org/10.1145/3053600.3053653](https://doi.org/10.1145/3053600.3053653)

**P49:** R. Chatley and T. Field and D. Wei, Continuous Performance Testing in Virtual Time. International Conference on Software Architecture Workshops, 2019. [https://doi.org/10.1109/ICSA-C.2019.00027](https://doi.org/10.1109/ICSA-C.2019.00027)

**P50:** D. Bardsley and L. Ryan and J. Howard, Serverless Performance and Optimization Strategies. International Conference on Smart Cloud, 2018. [https://doi.org/10.1109/SmartCloud.2018.00012](https://doi.org/10.1109/SmartCloud.2018.00012)

**P51:** M. G. Stochel and M. R. Wawrowski and J. J. Waskiel, Adaptive Agile Performance Modeling and Testing. IEEE Annual Computer Software and Applications Conference Workshops, 2012. [https://doi.org/10.1109/COMPSACW.2012.85](https://doi.org/10.1109/COMPSACW.2012.85)

**P52:** I. Epifani and C. Ghezzi and R. Mirandola and G. Tamburrelli, Model evolution by run-time parameter adaptation. International Conference on Software Engineering, 2009. [https://doi.org/10.1109/ICSE.2009.5070513](https://doi.org/10.1109/ICSE.2009.5070513)

**P53:** Y. Liu and I. Gorton and V. K. Le, A Configurable Event Correlation Architecture for Adaptive J2EE Applications. Australian Conference on Software Engineering, 2007. [https://doi.org/10.1109/ASWEC.2007.5](https://doi.org/10.1109/ASWEC.2007.5)

**P54:** G. Valetto and G. Kaiser, Using process technology to control and coordinate software adaptation. International Conference on Software Engineering, 2003. [https://doi.org/10.1109/ICSE.2003.1201206](https://doi.org/10.1109/ICSE.2003.1201206)
